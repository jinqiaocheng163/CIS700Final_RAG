{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7654c9e9-36bb-492e-b01e-83500d068b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# install & import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84df83-3b5f-467a-bfc9-d46617aeae1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ad07d-36cd-47b8-aed1-9333dac4b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c348b-e39b-4738-b915-412f8754dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb726dff-f37f-47be-8f83-cea2d9b7c31f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain_community\n",
    "!pip install chromadb\n",
    "!pip install sentence_transformers\n",
    "!pip install langchain\n",
    "!pip install langchain langchainhub httpx_sse\n",
    "!pip install langchain_openai\n",
    "!pip install azure-ai-documentintelligence\n",
    "!pip install pypdf\n",
    "!pip install flashrank\n",
    "!pip install torch\n",
    "!pip install cohere\n",
    "!pip install llama-index FlagEmbedding\n",
    "!pip install llama-index-postprocessor-flag-embedding-reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da5d3a-2a4c-41fc-9c62-f207fdf64a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install zai-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab9a2c-4f81-4fba-a4d4-919dac52c4d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a4758-d761-4681-b049-cec1b25732e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"langchain==0.1.20\" \"langchain-community==0.0.38\" --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1649936-5b65-4ca5-acb9-a7e434876f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb4f56-c690-4492-8f88-a3f2e81ad6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zai\n",
    "print(zai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb6be-a502-4a28-b6d7-d5872df7dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import concurrent.futures\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    AzureAIDocumentIntelligenceLoader,\n",
    ")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n",
    "\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1fd91-5281-41ef-9b41-ee76c379935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zai import ZhipuAiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3791d3-f23f-4661-9e06-5f3cf32e6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e2d67-81c3-4e28-b498-a34038e62bb3",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c7be6-d677-4ee3-8274-edf58bf3bd6a",
   "metadata": {},
   "source": [
    "## DATA FOLDER/ AZURE KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6afeb-69a0-4657-9e7d-a975bc4a4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"700data\"\n",
    "endpoint = \"https://1-297.cognitiveservices.azure.com/\"\n",
    "key = \"9f723c4c0b9245e98dd7a9dfd6d5667f.SUBdNWoWjIgmmpAe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f835a04-5515-4ae9-a18d-356c51fefe4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## use recursive text splitter, give each chunk a metadata, use Huggingface embeddingmodel to vectorize\n",
    "## support pdf, docx, txt files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622ff0f-8aaa-4d57-bc7f-0b5198f4582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(folder_path):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\",\n",
    "        \"\\u3002\",  \n",
    "        \"\",\n",
    "    ],\n",
    "    chunk_size=960,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    all_docs = []\n",
    "    documents = []\n",
    "\n",
    "    n=0;\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "        if filename.endswith(\".docx\"):  \n",
    "            docx_loader = AzureAIDocumentIntelligenceLoader(\n",
    "            api_endpoint=endpoint, api_key=key, file_path = file_path, api_model=\"prebuilt-layout\"\n",
    "        )\n",
    "            documents = docx_loader.load()\n",
    "        \n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            documents = PyPDFLoader(file_path).load()\n",
    "\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            documents = txt_loader.load()\n",
    "    \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                all_docs.append(Document(page_content=chunk, metadata={\"source\": filename.split('.')[0],\"index\": str(n)}))\n",
    "                n = n + 1\n",
    "            \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715f9b6-4d9d-416d-be4e-5bf36345c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bge-large-zh-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc72397-85a8-40e9-a168-679910e0b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = text_split(folder_path)\n",
    "# all_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24478e-2e98-4746-8815-3e3fa301692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95b137-a459-45df-ade7-a4d9a5b51db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs[3].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b2545-c4cc-4441-bd07-d1bab4ca4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312ad3e-9eb2-49d0-a0d9-1c5fb83958db",
   "metadata": {},
   "source": [
    "### save to local，persist directory，chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098087b6-16e4-4116-9ae0-ac35ec08021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "pa = \":/luanlai\"\n",
    "\n",
    "def w_v_db(doc, embed, pd):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "    documents=doc,\n",
    "    embedding=hf, \n",
    "    persist_directory= pd\n",
    "    )\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac70a5-a1e3-4fb6-a634-09a28e7fedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_v_db(pd, hf):\n",
    "    db= Chroma(persist_directory = pd , embedding_function=hf)\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918bb8e-6a2c-4b56-bf2c-ebc9918967a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = w_v_db(all_docs, hf, persist_directory)\n",
    "# vectorstore = r_v_db(persist_directory, hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bfcf8-7081-472a-942d-e70e63fae871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorstore._collection.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe82ba9-eca5-4c96-8fad-ce5ac10fdb92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## use CHATGLM, heres api and set rompt& conversation & memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6543d15-e573-4cda-b615-9a74e93c50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"glm-4.5\",\n",
    "    temperature=0.6,\n",
    "    openai_api_key=\"9f723c4c0b9245e98dd7a9dfd6d5667f.SUBdNWoWjIgmmpAe\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a medical expert assistant. Always answer in English. \"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab3ee1-a791-4b19-abe3-8ec5c1e63bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fde82-5cf5-4080-a83f-7849ae671968",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## clear memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104be47a-47a7-4b50-b29c-1fb66c4081d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoryclear(memory):\n",
    "    memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5750645-91d8-497a-b507-8db94abbe879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## use chroma similarity search and feed it back to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc4f24-1f9a-4122-bbc9-7a6bda35bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to prevent breast cancer?\"\n",
    "ans = vectorstore.similarity_search(query, 3)\n",
    "memoryclear(memory)\n",
    "conversation.invoke(query + str(ans[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc2fdd-fd00-4987-a5e4-23d4d1f2c988",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c87bac-2a60-4fb9-90d4-cf2048bd2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_test = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a medical expert assistant. Always answer in English. \"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_generate = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_test,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e075c2-1db1-4837-a617-869505aabd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Your task is to generate several different versions of the user’s question to help retrieve relevant documents from the vector database. By rephrasing the question from different perspectives, your goal is to overcome certain limitations of distance-based similarity search.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "mq_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=multiquery,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94456325-b13f-4f0f-b03f-d41b2e1582ef",
   "metadata": {},
   "source": [
    "## test QA set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d72074-00f2-48b1-94c8-93f1e58acb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def call_api(doc):\n",
    "    memoryclear(memory)\n",
    "    value = question_generate.invoke(doc.page_content)\n",
    "    return doc.metadata['index'], [value['text'], value['question'], doc.metadata['source']]\n",
    "\n",
    "chunk_questions = {}\n",
    "max_workers = 1\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    results = executor.map(call_api, all_docs)\n",
    "    \n",
    "    for index, data in results:\n",
    "        chunk_questions[index] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bab58-b88e-403f-aafd-918aae6e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392d2b0-1fae-4291-82ad-49ed86aac9e6",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ac132-ef28-4dcd-b25e-9588f1597ede",
   "metadata": {},
   "source": [
    "### write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbcc98-6d6b-42e9-951b-1d27c155b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub('\\x00','',text)\n",
    "\n",
    "with open(folder_path+'.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, escapechar = '\\\\')\n",
    "    writer.writerow(['Gnerated Q', 'Chunk', 'Source'])  \n",
    "    for key, value in chunk_questions.items():\n",
    "        clean_value = [clean_text(field) if isinstance(field, str) else field for field in value]\n",
    "        writer.writerow(clean_value)  \n",
    "print(\"Data has been written to something csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32208901-6831-4477-9268-6100d12027cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(chunk_questions) =\", len(chunk_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a89f5-2df7-4db7-833d-6b84c79d3bae",
   "metadata": {},
   "source": [
    "### csv into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2ceb4-8a9c-4aca-870d-706a1811b1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_csv(file_path):\n",
    "    chunk_questions = {}\n",
    "\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  \n",
    "\n",
    "        key_counter = 0  \n",
    "        for row in reader:\n",
    "            chunk_questions[str(key_counter)] = [row[0],row[1],row[2]]\n",
    "            key_counter += 1\n",
    "    \n",
    "    print(\"Data has been read from csv into dictionary.\")\n",
    "    return chunk_questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf3c7e-9ba4-44bd-946b-86714e2106ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_questions = extract_csv(folder_path+'.csv')\n",
    "print(len(chunk_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b65801-3e03-4714-9f5c-5bdd7deb1e19",
   "metadata": {},
   "source": [
    "## recall test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87011bbb-20fd-4a6b-ace8-981d3b5a21d2",
   "metadata": {},
   "source": [
    "### similarity_search and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed502c-4035-47ec-b55c-dfa3a312c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_metadata(query, db, top_k, meta):\n",
    "    query_lower = query.lower()\n",
    "    result = db.similarity_search(\n",
    "        query=query,\n",
    "        k= top_k,\n",
    "        filter={'source': meta} \n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa1ef7-b063-4956-9395-48da30a75196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correct_original(dic, mem, k, db):\n",
    "    hit = 0;\n",
    "    for key,value in dic.items():\n",
    "        x = hit\n",
    "        memoryclear(mem)\n",
    "        query = value[0]\n",
    "        result = db.similarity_search(query=query, k = k)\n",
    "        for doc in result:\n",
    "            #print(doc.metadata['index'])\n",
    "            #print(doc.metadata['source'])\n",
    "            if doc.metadata['index'] == key :\n",
    "                hit = hit + 1\n",
    "                break    \n",
    "        # if x == hit:\n",
    "        #     print(key)\n",
    "            \n",
    "    return hit\n",
    "\n",
    "print(correct_original(chunk_questions, memory, 1, vectorstore))\n",
    "print(correct_original(chunk_questions, memory, 3, vectorstore))\n",
    "print(correct_original(chunk_questions, memory, 5, vectorstore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb376e-9a72-411b-ae73-b2600c1a589d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correct_filtered(dic, mem, k, db):\n",
    "    hit = 0;\n",
    "    for key,value in dic.items():\n",
    "        x = hit\n",
    "        memoryclear(mem)\n",
    "        result = search_with_metadata(value[0], db, k, value[2])\n",
    "        for doc in result:\n",
    "            #print(doc.metadata['index'])\n",
    "            #print(doc.metadata['source'])\n",
    "            if doc.metadata['index'] == key :\n",
    "                hit = hit + 1\n",
    "                break\n",
    "        # if x == hit:\n",
    "        #     print(key)\n",
    "\n",
    "    return hit\n",
    "\n",
    "print(correct_filtered(chunk_questions, memory, 1, vectorstore))\n",
    "print(correct_filtered(chunk_questions, memory, 3, vectorstore))\n",
    "print(correct_filtered(chunk_questions, memory, 5, vectorstore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c705eab-6c05-4297-b6b6-895ee5159e49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ensemble retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf4aa0-ec80-4f49-a3d7-fea72ff2d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(all_docs)\n",
    "#chroma_vectorstore = Chroma.from_documents(all_docs, hf)\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import (\n",
    "     ElasticSearchBM25Retriever,\n",
    " )\n",
    "\n",
    "\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "\n",
    "chroma_retriever = vectorstore.as_retriever()\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5],\n",
    ")\n",
    "\n",
    "docs = ensemble_retriever.get_relevant_documents(query = \"breat cancer\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bdfce-28e1-4425-b851-6c0b94e2d003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def correct_ensemble(dic, mem, k, db):\n",
    "     hit = 0;\n",
    "     \n",
    "     for key,value in dic.items():\n",
    "         x = hit\n",
    "         memoryclear(mem)\n",
    "         query = value[0]\n",
    "         result = ensemble_retriever.get_relevant_documents(query = query)\n",
    "         result = result[:k]\n",
    "         for doc in result:\n",
    "             #print(doc.metadata['index'])\n",
    "             #print(doc.metadata['source'])\n",
    "             if doc.metadata['index'] == key :\n",
    "                 hit = hit + 1\n",
    "                 break    \n",
    "            \n",
    "     return hit\n",
    "\n",
    " print(correct_ensemble(chunk_questions, memory, 1, vectorstore))\n",
    " print(correct_ensemble(chunk_questions, memory, 3, vectorstore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c61d4d-5f77-4bf8-b310-7f1ca5e5da68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### chroma similarity + mmr search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187bfe7-7174-45cc-992a-832b55e5fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever.k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c537251-2f54-4927-94c3-da31ee3acccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ensemble_retrieval(query, top_k, db,meta):\n",
    "    all_results = []\n",
    "    result1 = search_with_metadata(query, db, top_k, meta)\n",
    "    result2 = bm25_retriever.invoke(query)\n",
    "    all_results.extend(result2)\n",
    "    all_results.extend(result1)\n",
    "    # for retriever in retrievers:\n",
    "    #     if retriever == 'search_with_metadata':\n",
    "    #         results = retriever(query, db, top_k, meta)\n",
    "    #     else:\n",
    "    #         results = retriever(query)\n",
    "    #     all_results.extend(results)\n",
    "    \n",
    "\n",
    "    \n",
    "    return all_results\n",
    "    \n",
    "# retrievers = [search_with_metadata, bm25_retriever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583379ff-8d2f-44f4-b474-94b8adccc3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ensemble_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af016f63-9d7c-45e9-ac77-cca22e196d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correct_ensem(dic, mem, k, db):\n",
    "    hit = 0;\n",
    "    for key,value in dic.items():\n",
    "        x = hit\n",
    "        memoryclear(mem)\n",
    "        # result = search_with_metadata(value[0], db, k, value[2])\n",
    "        result = ensemble_retrieval(value[0], 1, vectorstore, value[2])\n",
    "        for doc in result:\n",
    "            print(doc.metadata['index'])\n",
    "            print(doc.metadata['source'])\n",
    "            if doc.metadata['index'] == key :\n",
    "                hit = hit + 1\n",
    "                break\n",
    "        # if x == hit:\n",
    "        #     print(key)\n",
    "\n",
    "    return hit\n",
    "\n",
    "print(correct_ensem(chunk_questions, memory, 2, vectorstore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f0101-c548-466b-a5c8-5db9819d3f9a",
   "metadata": {},
   "source": [
    "### Multiquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6440061-ddc7-4b18-aa13-6242222de292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correct_mq(dic, mem, k, db):\n",
    "    hit = 0\n",
    "\n",
    "    for key, value in dic.items():\n",
    "        retrieved_chunks = set()  \n",
    "        memoryclear(mem)\n",
    "        querys = mq_chain.invoke(value[0])['text']\n",
    "        #qs = querys.split('.')\n",
    "        qs = []\n",
    "        qs.append(querys)\n",
    "        qs.append(value[0])\n",
    "        result = [search_with_metadata(q, db, k, value[2]) for q in qs]\n",
    "        exit_flag = False\n",
    "\n",
    "        # print(len(result))\n",
    "        # for re in result:\n",
    "        #     for r in re:\n",
    "        #         print(r)\n",
    "        #         print(\"111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\")\n",
    "        #     print(\"2222222222222222222222222222222222222222222222222222222222222222222222222222222\")\n",
    "        # break\n",
    "\n",
    "        for docs in result:\n",
    "            counter = 0\n",
    "            if exit_flag:\n",
    "                break\n",
    "            for doc in docs:\n",
    "                if counter == k:\n",
    "                    break\n",
    "                if doc.metadata['index'] in retrieved_chunks:\n",
    "                    continue\n",
    "                #print(doc.metadata['index'])\n",
    "                #print(doc.metadata['source'])\n",
    "                if doc.metadata['index'] == key:\n",
    "                    hit = hit + 1\n",
    "                    exit_flag = True\n",
    "                    break\n",
    "                retrieved_chunks.add(doc.metadata['index'])  \n",
    "                counter = counter + 1\n",
    "\n",
    "\n",
    "    return hit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7122a-ddd5-4603-8117-e9f08ec8aaa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(correct_mq(chunk_questions, memory, 1, vectorstore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76a02a-2ea1-4b7c-9ae8-757e496fb4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(correct_mq(chunk_questions, memory, 3, vectorstore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c319b-58b9-4941-a9f3-ddc7d91b8285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def correct_mq(dic, mem, k, db):\n",
    "#     hit = 0;\n",
    "#     for key,value in dic.items():\n",
    "#         x = hit\n",
    "#         memoryclear(mem)\n",
    "#         querys = mq_chain.invoke(value[0])\n",
    "#         query = querys['text']\n",
    "#         qs = query.split('.')\n",
    "#         qs.append(value[0])\n",
    "#         result = [search_with_metadata(q, db, k, value[2]) for q in qs]\n",
    "#         exit_flag = False\n",
    "#         for docs in result:\n",
    "#              if exit_flag:  \n",
    "#                 break\n",
    "#              for doc in docs:\n",
    "#                 print(doc.metadata['index'])\n",
    "#                 print(doc.metadata['source'])\n",
    "#                 if doc.metadata['index'] == key :\n",
    "#                     hit = hit + 1\n",
    "#                     exit_flag = True\n",
    "#                     break\n",
    "#         # if x == hit:\n",
    "#         #     print(key)\n",
    "\n",
    "#     return hit\n",
    "\n",
    "# print(correct_mq(chunk_questions, memory, 1, vectorstore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a736eb-f2f9-430f-aa82-fada568294cc",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb1750-9199-4e78-a1e5-7f04cd044656",
   "metadata": {},
   "source": [
    "### BGE RERANKER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41480c7d-bcd2-43f5-ac98-f07ae7038e21",
   "metadata": {},
   "source": [
    "#### BGE RERANKER M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28c575-22f5-4d39-b58f-bbbf476dd245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker = FlagEmbeddingReranker(\n",
    "#     top_n= 1,\n",
    "#     model=\"BAAI/bge-reranker-v2-m3\",\n",
    "#     use_fp16=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f7728-bc93-4a87-9753-8ca93eaa7bc3",
   "metadata": {},
   "source": [
    "#### BGE RERANKER LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ae05c-c8ab-42dd-a08a-db03d493509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker = FlagEmbeddingReranker(\n",
    "#     top_n= 5,\n",
    "#     model=\"BAAI/bge-reranker-large\",\n",
    "#     use_fp16=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f58426-6a7d-44f9-a503-fa5beeb826d9",
   "metadata": {},
   "source": [
    "#### BGE RERANKER MULTILAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bef9bc-0b19-4a08-8f61-c54e956d925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d955e0-0603-4a4d-a00a-534f2ad9c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker = FlagEmbeddingReranker(\n",
    "#     top_n= 1,\n",
    "#     model = \"BAAIbge-reranker-v2-minicpm-layerwise/bge-reranker-v2-minicpm-layerwise\",\n",
    "#     use_fp16=True\n",
    "# )\n",
    "\n",
    "from FlagEmbedding import LayerWiseFlagLLMReranker\n",
    "\n",
    "reranker = LayerWiseFlagLLMReranker(\n",
    "    model_name_or_path=\"BAAI/bge-reranker-v2-minicpm-layerwise\",\n",
    "    use_fp16=True,\n",
    "    trust_remote_code=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07c9cf-ba5d-402b-aa30-1e18091fde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bge_rerank_multilayer(dic, mem,k):\n",
    "    hit = 0\n",
    "    count = 0\n",
    "    for key, value in dic.items():\n",
    "        #print(count)\n",
    "        count = count + 1\n",
    "        memoryclear(mem)\n",
    "        query = value[0]\n",
    "        result = search_with_metadata(query, vectorstore, 5, value[2])\n",
    "        documents = [re.page_content for re in result]\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        scores = reranker.compute_score(pairs, cutoff_layers=[28])\n",
    "\n",
    "        top_scores_indices = sorted(\n",
    "            range(len(scores)),\n",
    "            key=lambda i: scores[i],\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "        \n",
    "        for index in top_scores_indices:\n",
    "            if documents[index] == value[1]:\n",
    "                hit += 1\n",
    "                print(hit)\n",
    "                break\n",
    "\n",
    "    return hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b578b35-4f9c-4950-94ec-a2b9ae57ac89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bge_rerank_multilayer(chunk_questions, memory, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d510fc-f8b5-4bb7-849f-78d03018bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bge_rerank_multilayer(chunk_questions, memory, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e1c9d-a27e-482d-8676-b729b2bcbd80",
   "metadata": {},
   "source": [
    "#### RERANK function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5733100-bd0e-45a8-b619-1705a53f3153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def bge_rerank(dic, mem, vectorstore):\n",
    "#     hit = 0\n",
    "#     count = 0\n",
    "#     for key, value in dic.items():\n",
    "#         print(count)\n",
    "#         count = count + 1\n",
    "#         memoryclear(mem)\n",
    "#         query = value[0]\n",
    "#         result = search_with_metadata(query, vectorstore, 5, value[2])\n",
    "#         documents = [re.page_content for re in result]\n",
    "#         nodes = [NodeWithScore(node=TextNode(text=doc)) for doc in documents]\n",
    "#         query_bundle = QueryBundle(query_str=query)\n",
    "#         ranked_nodes = reranker._postprocess_nodes(nodes, query_bundle)\n",
    "#         for node in ranked_nodes:\n",
    "#             # node.node.get_content(),\n",
    "#             print(\"-> Score:\", node.score)\n",
    "#             if node.node.get_content() == value[1]:\n",
    "#                 hit += 1\n",
    "#                 break\n",
    "#     return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33e918-66fb-4e28-880f-f728d121db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bge_rerank(chunk_questions, memory, vectorstore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d0368-49f5-4a51-aad7-11bc60edd176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_bundle = QueryBundle(query_str=query)\n",
    "# ranked_nodes = reranker._postprocess_nodes(nodes, query_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523aa40-0a3e-42f8-80d7-a9c1c97ab1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in ranked_nodes:\n",
    "#     print(node.node.get_content(), \"-> Score:\", node.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7b5bb-6a4e-447d-b75f-18cc5a408457",
   "metadata": {},
   "source": [
    "### Cohere Reranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dd159-f663-4b92-a6e6-d605bbfcca47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(\"quwDJP3BjSin11gf53PUFyjwV01o6oAu5QnazGxJ\")\n",
    "\n",
    "def rerank_with_cohere(query, results, k): \n",
    "    texts = [result.page_content for result in results]\n",
    "    reranked = co.rerank(model='rerank-multilingual-v3.0', query=query, documents=texts, top_n = k, return_documents = True)\n",
    "\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c058e5f-373a-406e-bc0d-db33f8722893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = value[0] + value[2].split('.')[0]\n",
    "# qe =\"GPU ？\"\n",
    "# re = rerank_with_cohere(qe, search_with_metadata(qe, vectorstore, 5), 5)\n",
    "# print(re.results[2].document.text)\n",
    "\n",
    "# print(re.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6b9ab-69b4-446a-af3f-8627d7173d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COHERE reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb53155-a21f-404c-8855-cbff7e85875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute):\n",
    "        self.calls_per_minute = calls_per_minute\n",
    "        self.calls_made = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def wait(self):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if self.calls_made >= self.calls_per_minute:\n",
    "            time_to_wait = 80 - elapsed_time\n",
    "            if time_to_wait > 0:\n",
    "                time.sleep(time_to_wait)\n",
    "            self.calls_made = 0\n",
    "            self.start_time = time.time()\n",
    "        self.calls_made += 1\n",
    "\n",
    "# RateLimiter\n",
    "def correct_rerank(dic, mem, k):\n",
    "    hit = 0\n",
    "    rate_limiter = RateLimiter(10)  \n",
    "    for key, value in dic.items():  \n",
    "        memoryclear(mem)\n",
    "        query = value[0]\n",
    "        result = search_with_metadata(query, vectorstore, 5, value[2])\n",
    "        rate_limiter.wait()  \n",
    "        reranked = rerank_with_cohere(query, result, k)\n",
    "        for a in reranked.results:\n",
    "            if a.document.text == value[1]:\n",
    "                hit += 1\n",
    "                break\n",
    "    return hit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b8ff7a-1a5e-4543-8db5-eb7b4e08dcda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(correct_rerank(chunk_questions, memory, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3be21-26d9-4f42-b0a3-6a89265a62f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
